{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_akA0-FThgcI"
      },
      "source": [
        "# Document Retrieval (40 pts)\n",
        "\n",
        "Information Retrieval is the process of obtaining relevant information from a system based on a user’s query. One common example is retrieving documents that match a query from a large corpus. In the vector space model, documents are represented as vectors, and a query can also be represented as a vector. To find documents that are relevant to the query, we can measure the similarity between the query vector and the document vectors, using methods such as cosine similarity: <br><br>\n",
        "\n",
        "$$cosine\\,similarity = cos(\\theta) = \\frac{q \\cdot d}{\\|q\\| \\|d\\|}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2OQTWFliY5m"
      },
      "source": [
        "where **q** and **d** are the query and document vectors respectively.\n",
        "How are these vector space models built? Let us represent a corpus of documents as a term-document matrix. In the term-document matrix, rows correspond to documents in the corpus and\n",
        "columns correspond to terms(words). The entries in the matrix can be defined in different ways (we will describe\n",
        "a few variations in this assignment question). A term document matrix allows for a way to index several\n",
        "documents against which a user query can be compared to fetch relevant documents.  \n",
        "\n",
        "Consider a corpus of documents:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"THE CAT IS ON THE MAT\",\n",
        "    \"A DOG IS IN THAT GARDEN\",\n",
        "    \"HE LIVES IN THE HOUSE WITH A GARDEN\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The term-document matrix of this corpus would be seem like this:\n",
        "\n",
        "|        | THE | CAT | IS | ON | MAT | A | DOG | IN | THAT | GARDEN | HE | LIVES | HOUSE | WITH |\n",
        "|--------|-----|-----|----|----|-----|---|-----|----|------|--------|----|-------|-------|------|\n",
        "|  doc1  |     |     |    |    |     |   |     |    |      |        |    |       |       |      |      \n",
        "|  doc2  |     |     |    |    |     |   |     |    |      |        |    |       |       |      |\n",
        "|  doc3  |     |     |    |    |     |   |     |    |      |        |    |       |       |      |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cautions\n",
        "* Implement the solution **without** importing additional libraries.\n",
        "* Do **not** modify any part of the code that is not explicitly marked with “YOUR CODE.”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0yxKNb5hbI6",
        "outputId": "6161b3c6-175d-4439-9357-01a74ab8c67b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy as sp\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGeSrhInhbtD"
      },
      "source": [
        "## Problem 1. Simple Word Counts (15 pts)\n",
        "In this problem, we will fill the entries of the term-document matrix using simple word counts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpNXKiO4hbI8"
      },
      "source": [
        "### (a) Complete the <code>simple_word_counts</code> function. (5 pts)\n",
        "\n",
        "**Instructions**\n",
        "\n",
        "The function returns a term-document matrix for a given corpus using simple word counts (the number of times a word appears in a document)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVr82GxFhbI9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 2, 0],\n",
              "       [1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0],\n",
              "       [1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1]])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def simple_word_counts(corpus: list[str]) -> np.ndarray:\n",
        "    \n",
        "    term_doc_matrix = None\n",
        "\n",
        "    ###############Your Code################\n",
        "    # The order of words in the term-document matrix DOES NOT matter\n",
        "    # Replace \"pass\" statement with your code\n",
        "    vocabulary = sorted(set(word.lower() for doc in corpus for word in doc.split()))\n",
        "    word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "\n",
        "    term_doc_matrix = np.zeros((len(corpus), len(vocabulary)), dtype=int)\n",
        "\n",
        "    for doc_idx, doc in enumerate(corpus):\n",
        "        for word in doc.split(): \n",
        "            word = word.lower()\n",
        "            if word in word2idx:\n",
        "                term_doc_matrix[doc_idx, word2idx[word]] += 1\n",
        "    ########################################\n",
        "\n",
        "    return term_doc_matrix\n",
        "\n",
        "\n",
        "term_doc_matrix = simple_word_counts(corpus)\n",
        "term_doc_matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJqfCigjhbI-"
      },
      "source": [
        "### (b) Complete the <code>compute_sim</code> function. (5 pts)\n",
        "\n",
        "**Instructions**\n",
        "\n",
        "Using the simple word counts, the function computes the cosine similarity between a user query and each document in a given corpus.\n",
        "The function returns the similarity scores for all documents in the corpus based on the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "X8BmmokAhbI-"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.5      , 0.4330127, 0.625    ])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def cos_distance(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    return 1 - sp.spatial.distance.cosine(a, b)\n",
        "\n",
        "\n",
        "def compute_sim(corpus: list[str], query: str) -> np.ndarray:\n",
        "    \n",
        "    cos_sim = None\n",
        "    \n",
        "    ###############Your Code################\n",
        "    # Make use of cos_distance function above!\n",
        "    # The result would be a 1D ndarray with shape (3, )\n",
        "    # Replace \"pass\" statement with your code    \n",
        "    query_words = query.lower().split()\n",
        "    term_doc_matrix = simple_word_counts(corpus)\n",
        "    vocabulary = sorted(set(word.lower() for doc in corpus for word in doc.split()))\n",
        "    word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "\n",
        "    query_vector = np.zeros(term_doc_matrix.shape[1], dtype=int)\n",
        "    for word in query_words:\n",
        "        if word in vocabulary:\n",
        "            idx = word2idx[word]\n",
        "            query_vector[idx] += 1\n",
        "            \n",
        "    cos_sim = np.zeros(len(corpus), dtype=float)\n",
        "    for i in range(len(corpus)):\n",
        "        cos_sim[i] = cos_distance(term_doc_matrix[i], query_vector)\n",
        "    cos_sim = np.array(cos_sim, dtype=float)  # Ensure the result is a numpy array\n",
        "    \n",
        "    ########################################\n",
        "    \n",
        "    return cos_sim\n",
        "\n",
        "query = \"THE DOG LIVES IN THE GARDEN\"\n",
        "compute_sim(corpus, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb6jGth9l3H7"
      },
      "source": [
        "### (c) Discuss the limitations of using simple word counts to create document vectors. (5 pts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7CKHxFVmI0K"
      },
      "source": [
        "Answer : \n",
        "1. 관사 등의 카운트가 포함됨.\n",
        "2. 문서가 길면 유사도가 높아짐.\n",
        "3. 유의어들을 다 따로 카운트함.\n",
        "4. 전체 집합이 커지면 문서 대부분의 값이 0으로 수렴함\n",
        "5. 문맥을 반영하지 못 함."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G5T_oglmOwj"
      },
      "source": [
        "## Problem 2. TF-IDF (25 pts)\n",
        "\n",
        "In this section, you will explore Term Frequency-Inverse Document Frequency (TF-IDF) as an improvement over simple word counts.\n",
        "\n",
        "TF-IDF involves multiplying the term frequency (which can be the simple word counts) by the inverse document frequency (IDF). The IDF measures how informative a word is, indicating whether it is common or rare across the entire corpus. It is calculated as the logarithm of the inverse of the fraction of documents that contain the word (i.e., the total number of documents divided by the number of documents in which the word appears, followed by taking the logarithm of this ratio).\n",
        "<br><br>\n",
        "\n",
        "$$idf(t) = log_2 \\frac{(Number\\,\\,of\\,\\,documents\\,\\,in\\,\\,the\\,\\,corpus)}{(Number\\,\\,of\\,\\,documents\\,\\,t\\,\\,appears\\,\\,in)}$$  \n",
        "  <br>\n",
        "  \n",
        "$$tf-idf \\,\\,=\\,\\, tf \\times idf$$\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (a) Complete the <code>idf</code> function. (5 pts)\n",
        "\n",
        "The function returns a IDF for a given corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.5849625, 1.5849625, 1.5849625, 0.5849625, 1.5849625, 1.5849625,\n",
              "       0.5849625, 0.5849625, 1.5849625, 1.5849625, 1.5849625, 1.5849625,\n",
              "       0.5849625, 1.5849625])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def idf(corpus: list[str]) -> np.ndarray:\n",
        "    \n",
        "    idf_doc = None\n",
        "\n",
        "    ###############Your Code################\n",
        "    # NOTE the base of log is 2, not 10 or e\n",
        "    # The result would be a 1D ndarray with shape (14, )\n",
        "    # Replace \"pass\" statement with your code\n",
        "    vocabulary = sorted(set(word.lower() for doc in corpus for word in doc.split()))\n",
        "    word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "    term_doc_matrix = simple_word_counts(corpus)\n",
        "\n",
        "    idf_doc = np.zeros(len(vocabulary), dtype=float)\n",
        "    num_docs = len(corpus)\n",
        "\n",
        "    for word, idx in word2idx.items():\n",
        "        doc_freq = np.sum(term_doc_matrix[:, idx] > 0)\n",
        "        if doc_freq > 0:\n",
        "            idf_doc[idx] = np.log2(num_docs / doc_freq)\n",
        "        else:\n",
        "            idf_doc[idx] = 0.0\n",
        "    idf_doc = np.array(idf_doc, dtype=float)  # Ensure the result is a numpy array\n",
        "    \n",
        "    ########################################\n",
        "\n",
        "    return idf_doc\n",
        "\n",
        "\n",
        "idf_val = idf(corpus)\n",
        "idf_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.       , 1.5849625, 0.       , 0.       , 0.       , 0.       ,\n",
              "        0.       , 0.5849625, 0.       , 1.5849625, 1.5849625, 0.       ,\n",
              "        1.169925 , 0.       ],\n",
              "       [0.5849625, 0.       , 1.5849625, 0.5849625, 0.       , 0.       ,\n",
              "        0.5849625, 0.5849625, 0.       , 0.       , 0.       , 1.5849625,\n",
              "        0.       , 0.       ],\n",
              "       [0.5849625, 0.       , 0.       , 0.5849625, 1.5849625, 1.5849625,\n",
              "        0.5849625, 0.       , 1.5849625, 0.       , 0.       , 0.       ,\n",
              "        0.5849625, 1.5849625]])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf_idf_matrix = term_doc_matrix * idf_val\n",
        "tf_idf_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUGKfTHahbI-"
      },
      "source": [
        "### (b) Complete the <code>compute_sim_tfidf</code> function. (5 pts)\n",
        "\n",
        "\n",
        "Using TF-IDF, the function computes the cosine similarity between a user query and each document in a given corpus.\n",
        "The function returns the similarity scores for all documents in the corpus based on the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VmKsS9e1hbI_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.16919074, 0.47521095, 0.43172986])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def compute_sim_tfidf(corpus: list[str], query: str) -> np.ndarray:\n",
        "    \n",
        "    cos_sim = None\n",
        "\n",
        "    ###############Your Code################\n",
        "    # The result would be a 1D ndarray with shape (3, )    \n",
        "    # Replace \"pass\" statement with your code\n",
        "    query_words = query.lower().split()\n",
        "    term_doc_matrix = simple_word_counts(corpus)\n",
        "    idf_val = idf(corpus)\n",
        "    vocabulary = sorted(set(word.lower() for doc in corpus for word in doc.split()))\n",
        "    word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "\n",
        "    query_vector = np.zeros(term_doc_matrix.shape[1], dtype=float)\n",
        "    for word in query_words:\n",
        "        if word in vocabulary:\n",
        "            idx = word2idx[word]\n",
        "            query_vector[idx] += 1\n",
        "    \n",
        "    query_vector *= idf_val  # Apply IDF to the query vector\n",
        "    cos_sim = np.zeros(len(corpus), dtype=float)\n",
        "    for i in range(len(corpus)):\n",
        "        cos_sim[i] = cos_distance(tf_idf_matrix[i], query_vector)\n",
        "    cos_sim = np.array(cos_sim, dtype=float)  # Ensure the result is a numpy array\n",
        "        \n",
        "    ########################################\n",
        "    \n",
        "    return cos_sim\n",
        "\n",
        "\n",
        "compute_sim_tfidf(corpus, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXKOCvD9m2Ao"
      },
      "source": [
        "### (c) Explain how the TF-IDF representation addresses the issues identified in Problem 1(c). (5 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxQ1deSvm_0q"
      },
      "source": [
        "Answer :\n",
        "1. 관사 등의 의미 없는 흔한 단어의 IDF 값이 작게 계산되어 영향이 줄어듦.\n",
        "2. 상대 빈도를 계산하기 때문에 문서의 길이에 비교적 자유로움.\n",
        "3. 드물게 등장하는 단어의 계산값이 높아져 정보량이 높은 단어를 강조함."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTFwI7jShbI_"
      },
      "source": [
        "### (d) Complete the <code>compute_sim_tfidf_L2</code> function. (5 pts)\n",
        "\n",
        "Instead of cosine similarity, the function computes the L2 distance between a user query and each document using TF-IDF.\n",
        "The function returns the L2 distances for all documents in the corpus based on the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pzirO7bchbI_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1.28903783, 1.02448919, 1.06608644])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def compute_sim_tfidf_l2(corpus: list[str], query: str) -> np.ndarray:\n",
        "    \n",
        "    l2_dist = None\n",
        "\n",
        "    ###############Your Code################\n",
        "    # The result would be a 1D ndarray with shape (3, )\n",
        "    # Replace \"pass\" statement with your code\n",
        "    query_words = query.lower().split()\n",
        "    term_doc_matrix = simple_word_counts(corpus)\n",
        "    idf_val = idf(corpus)\n",
        "    vocabulary = sorted(set(word.lower() for doc in corpus for word in doc.split()))\n",
        "    word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "    query_vector = np.zeros(term_doc_matrix.shape[1], dtype=float)\n",
        "\n",
        "    for word in query_words:\n",
        "        if word in vocabulary:\n",
        "            idx = word2idx[word]\n",
        "            query_vector[idx] += 1\n",
        "    query_vector *= idf_val  # Apply IDF to the query vector\n",
        "    query_vector /= np.linalg.norm(query_vector)  # Normalize the query vector\n",
        "\n",
        "    tf_idf_matrix = term_doc_matrix * idf_val\n",
        "\n",
        "    l2_dist = np.zeros(len(corpus), dtype=float)\n",
        "    for i in range(len(corpus)):\n",
        "        doc_vector = tf_idf_matrix[i]\n",
        "        doc_vector /= np.linalg.norm(doc_vector)\n",
        "        l2_dist[i] = np.linalg.norm(doc_vector - query_vector)\n",
        "    l2_dist = np.array(l2_dist, dtype=float)  # Ensure the result is a numpy array \n",
        "    ########################################\n",
        "    \n",
        "    return l2_dist\n",
        "\n",
        "\n",
        "compute_sim_tfidf_l2(corpus, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tMbC9-SraCm"
      },
      "source": [
        "### (e) Compare the performance of cosine similarity and L2 distance in this task. Which one do you think works better and why? (5 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4BIFbbenISn"
      },
      "source": [
        "Answer :\n",
        "I would say cosine similarity work better than L2 distance. The given query is more about 2nd and 3rd doc in corpus and cosine similarity represents it well. On the other hand, L2 distance says that 1st doc is much similar with query than others which is wrong."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "cGeSrhInhbtD",
        "0G5T_oglmOwj"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
